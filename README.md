Retrieval-Augmented Generation (RAG) using LLMs

An end-to-end AI-powered medical chatbot that answers user queries by retrieving relevant information from medical literature and generating accurate, context-aware responses using Large Language Models.


Overview

This project implements a Retrieval-Augmented Generation (RAG) pipeline over medical documents.
Instead of relying solely on a language modelâ€™s memory, the system retrieves relevant medical knowledge from a PDF corpus and uses it as context to generate reliable answers.

The chatbot is designed with:

Scalability

Modularity

Reproducibility

Research-oriented system design


Key Features

1. PDF-based medical knowledge ingestion

2. Semantic search using vector embeddings

3. Context-aware answers with LLMs

4. Fast similarity search using FAISS

5. Interactive Flask web interface

6. Modern dark-themed UI

7. Modular and extensible codebase


 System Architecture

Medical PDF
    â†“
Document Loader
    â†“
Text Chunking
    â†“
Embedding Generation
    â†“
FAISS Vector Store
    â†“
Retriever
    â†“
LLM (Answer Generation)
    â†“
Flask Web App (Chat Interface)

Demo & Screenshots
 Chat Interface Screenshot

(Add a screenshot of the chatbot UI here)

![Chatbot UI](screenshots/chat_ui.png)

ğŸ”¹ Screen Recording (Demo)

(Add a short demo video or GIF here)



Tech Stack
Backend & AI

Python

LangChain

FAISS

Hugging Face Embeddings

Open-source LLMs


Web Framework

Flask

HTML / CSS / JavaScript


Tools & Libraries

Conda (environment management)

NumPy

PyPDF

Git


Project Structure
â”œâ”€â”€ data
â”‚   â””â”€â”€ Medical_book.pdf
â”‚
â”œâ”€â”€ research
â”‚   â””â”€â”€ trials.ipynb
â”‚
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ helper.py
â”‚   â””â”€â”€ prompt.py
â”‚
â”œâ”€â”€ static
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ templates
â”‚   â””â”€â”€ chat.html
â”‚
â”œâ”€â”€ faiss_index
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ template.sh
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

How It Works

Document Loading
Medical PDFs are loaded and parsed into raw text.

Text Chunking
Documents are split into smaller, overlapping chunks for better semantic retrieval.

Embedding Generation
Each chunk is converted into a dense vector representation.

Vector Storage
Embeddings are stored in a FAISS vector database for efficient similarity search.

Query Processing
User queries are embedded and matched against the vector store.

Answer Generation
The most relevant chunks are passed to the LLM to generate grounded answers.

ğŸš€ Getting Started
1ï¸âƒ£ Create Conda Environment
conda create -n rag-llm python=3.10
conda activate rag-llm

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Build Vector Store (if not already created)

Run the notebook:

research/trials.ipynb

4ï¸âƒ£ Run the Application
python app.py

5ï¸âƒ£ Open in Browser
http://127.0.0.1:5000

ğŸ’¬ Sample Queries

What are the symptoms of diabetes?

Explain hypertension in simple terms.

What are common treatments for asthma?

ğŸ”’ Disclaimer

âš ï¸ This chatbot is for educational and research purposes only.
It is not a substitute for professional medical advice.

ğŸ“ˆ Future Enhancements

âœ… Source citations with each response

âœ… Chat memory for multi-turn conversations

âœ… Evaluation metrics for retrieval quality

âœ… Deployment to cloud platforms (AWS / Render / HF Spaces)

âœ… Support for multiple document uploads

ğŸ‘©â€ğŸ’» Author

Srishti Singh
ğŸ“ GitHub: https://github.com/SrishtiSingh100

ğŸ“ LinkedIn: https://www.linkedin.com/in/srishtisingh01/


